\documentclass[11pt,a4paper]{article}
\usepackage[latin1]{inputenc}
\usepackage[norsk]{babel}
\usepackage{verbatim}
\usepackage{alltt}
\usepackage{graphicx}
%remove the following line if not using special symbols
\usepackage{amssymb}
\setlength{\parindent}{0pt}
\setlength{\parskip}{2ex plus 0.5ex minus 0.2ex}
\begin{document}
\title{Hovedprosjekt 08}
\author{M Byhring, T E Iversen, L M Bredal\\
Høgskolen i Oslo, avdeling for ingeniørutdanning}
\renewcommand{\today}{26 februar, 2008}
\maketitle


\newpage 

\section{Forord}

Dette dokumentet forteller om planleggingen og arbeidsmetodene som er brukt i hovedprosjektet. 

Gruppen kom i kontakt med arbeidsgiver gjennom en foreleser på Høgskolen i Oslo. Arbeidsgiveren er professor på University of Edinburgh, og bruker et system for å dele ut konfigurasjonsinnstillinger til et stort nettverk av maskiner ved hjelp av et konsollprogram. Han har for tiden lite oversikt over eventuelle konfigurasjonsavvik, og vil derfor ha muligheten til å få et grafisk grensesnitt over dette.

Hensikten med oppgaven er å visualisere nevnte avvik og/eller feil på en oversiktlig og brukervennlig måte.

Denne rapporten er optimalisert for papir, og det kreves noe kompetanse innen matematikk og informatikk for å forstå innholdet.

Vi ønsker å rette en stor takk til følgende personer som har hjulpet og veiledet oss i gjennom prosjektet:
\begin{itemize}
\item[$\triangleright$] Simen Hagen - for veiledning gjennom hele prosjektperioden, og for prosjektfunn
\end{itemize}

\newpage
\setlength{\parskip}{0ex}
\tableofcontents
\setlength{\parskip}{2ex plus 0.5ex minus 0.2ex}
\newpage
%TODO: Flytte tableofcontents ned til rett over hoveddel jfr dokumentasjonsstandard???

%\subsection{Sammendrag}
%Dette avsnittet vil bli brukt hvis vi føler det nødvendig å ta med et sammendrag av hele prosessrapporten når den er ferdig skrevet.
\newpage


\section{Innledning}

\subsection{Om oppdragsgiver}
Oppdragsgiver er Høgskolen i Oslo, ved lektor Simen Hagen.
Simen har tidligere hatt kontakt med Paul Anderson, professor ved University of Edinburgh. De har snakket om et mulig prosjekt som går ut på å visualisere konfigurasjonsfiler.
%her kommer alt sakset fra forprosjektrapporten
\subsection{Dagens situasjon}
LCFG (Local ConFiGuration system) er et system for å administrere konfigurasjon av et stort antall UNIX-maskiner. Systemet er utviklet av blant annet Paul Anderson ved University of Edinburgh.
Konfigurasjonsinnstillingene distribueres ved hjelp av XML-filer (Extendible Markup Language) som hentes fra en webserver. Hver maskin har sin XML-fil med en unik konfigurasjon for den maskinen.
Oppdragsgiver ønsker å få en grafisk oversikt over maskiner i nettverket, der disse er gruppert basert på hvor forskjellig maskinene er konfigurert. 
\subsection{Mål og rammebetingelser}

\subsubsection{Mål}
Hovedmålet med prosjektet er å ekstrahere data og visualisere grupper av maskiner basert på konfigurasjons(u)likhet.
Vårt system skal kunne:
\begin{itemize}
\item[$\triangleright$]  Lese inn de dataene vi har spesifisert fra XML-profilene og legge inn i en database.
\item[$\triangleright$] Hente de dataene vi har spesifisert fra databasen.
\item[$\triangleright$] Visualisere data på en eller flere hensiktsmessige måter.
\end{itemize}

\subsubsection{Rammebetingelser}

Oppgaven er løst formulert fra oppdragsgivers side.
Det er ikke stilt spesielle krav til hvilke data som skal visualiseres eller hvilken visualiseringsmetode som skal brukes. 
Vi har ca 1060 konfigurasjonsfiler og vi beregner at noe av tiden vi har til rådighet vil gå med til å identifisere hva dataene representerer, og finne hvilke data vi bør konsentrere oss om.
På grunn av den store datamengden vil det i første omgang være naturlig å velge ut noen få parametere som vi bruker til visualisering, men systemet bør være skalerbart.
Vi har også fått frie hender til valg av utviklingsplattform og verktøy.
%end forprosjektrapport

%Det kan tenkes at innledningen på Hoveddelen ikke får noen tittel, slik som den har nå. Vi får se.

\section{Planlegging og metode}

\subsection{Datagrunnlag}
Datagrunnlaget for prosjektet er et sett XML-filer (Extensible Markup Language), der hver fil representerer konfigurasjonen til en maskin på et gitt tidspunkt. 
Første datasett bestod av 1060 XML-filer med en gjennomsnittlig størrelse på 1 MB.
Fase en av prosjektet gikk i stor grad ut på å få en oversikt over strukturen på disse filene og hvilke data vi hadde tilgjengelig.
Oppdragsgiver hadde gitt oss en viss pekepinn på hvilke data som ikke var interessante, blant annet feilsøkingsinformasjon.
Dette ble fjernet, noe som halverte filstørrelsen og gjorde det lettere å lese filene manuelt.
%Disse filene kan endres over tid, slik at det er mulig å lage en historikk over endringer i konfigurasjonsprofilen dersom man tar vare på gamle data.
%
 %Vi startet med en samling på 1060 XML-filer der hver fil hadde en gjennomsnittlig størrelse på omkring 1MB. 
%Første skritt var å fjerne en del data som av oppdragsgiver var angitt som uinteressante for oss, da de kun var beregnet på feilsøking. 
%\\
%\\
%Vi kunne nå begynne å skaffe oss et bilde av datagrunnlaget ved å ta for oss en av filene og se på strukturen. Etter å ha fått en viss oversikt over strukturen kunne vi lage script for å hente ut den informasjonen vi ønsket fra alle filene for å skaffe oss en total oversikt over materialet. 
Neste skritt i prosessen var å velge ut hvilke data vi ville jobbe videre med. 
I dette arbeidet benyttet vi oss av dokumentasjonen \cite{lcfgGuide} til LCFG for å finne ut hva de enkelte verdiene representerte. Denne dokumentasjonen var forholdsvis mangelfull, men  den var likevel til stor hjelp. Det viste seg blant annet at en del datafelter vi i utgangspunktet så på som interessante, likevel ikke hadde den betydningen vi trodde.
%\textbf{(eksempler?)}  Vi trodde Serverkomponenten betød at maskinen var en server, men den angir kun at den er en LCFG-server og sier ikke noe om andre tjenester.
%\\
%\\
En oppsummering av oppbygningen til XML-filene:
Profilen består av to hovedseksjoner: components og packages.
Seksjonen components inneholder en rekke underseksjoner som representerer konfigurasjonen av et program, eller tjeneste (komponent) på maskinen.
En komponent kan inneholde både andre underseksjoner og bladnoder som inneholder informasjon.
Det er mer enn 100 forskjellige 'components' i datamaterialet vårt.
Det er kun et obligatorisk felt i filene: 'profile'-komponenten som må være med i componentsseksjonen.

%TODO:
%Flere seksjoner:

\subsection{Verktøy}
Tidlig i prosjektprosessen hadde vi møte med veileder hvor vi diskuterte hvilke teknologier vi burde bruke til å gjennomføre vårt prosjekt. Vi trengte et programmeringsspråk som raskt kunne tolke XML-data, kommunisere med en database og generere tekstfiler.
%er ekstra kjapt og hendig med XML-data (Extensible Markup Language), et som egner seg til å ekstrahere ut data fra en DB, et som egner seg til å generere filer 
Vi trengte også et språk som skal brukes til å visualisere hundrevis av noder i et tredimensjonalt rom. 
Da XML-filene er svært store, vil vi også legge inn ønsket informasjon inn i en database, både for å spare plass og tid.
Etter å ha lest om forskjellige teknologier og vurdert alternativene, bestemte vi oss for å bruke Perl, VRML (Virtual Reality Modeling Language) og mySQL. 
%Etter råd fra veileder var det klart hvilke to språk som skulle være våre hovedverktøy; Perl og VRML (Virtual Reality Modeling Language). Perl er ypperlig til å innsette og ekstrahere data, og til å generere *.wrl-filer. Slik vi har tenkt oss å visualisere konfigurasjonen i LCFG, vil VRML fylle alle våre krav til framvisning av dette.
%Når vi startet prosjektet hadde gruppen noe erfaring med Perl. Bruksområdene til Perl i vårt prosjekt vil være ekstrahering av data fra XML og database, og generering av filer. Det vi tidligere har gjort i Perl , mens vi nå jobber mot XML, database og generering av filer. Siden vi alle på gruppen har jobbet litt med Perl, har det ikke vært noen stor jobb å tilegne seg mer kunnskap i disse områdene. %TODO : bør vi endre dette litt..?%

VRML er et Markup-Language med en syntaks som ligner på HTML (HyperText Markup Language) og XML. 

Gruppa har valgt å benytte mySQL som databasemotor, da vi tidligere har jobbet med denne.
Som programmerings-IDE har vi valgt å bruke eclipse, med tilleggsmodulene EPIC (Eclipse Perl Integration) og subclipse for versjonskontroll (Subversion). Dette fordi det er et godt utviklingsmiljø til Perl, med integrert støtte for subversion.

Som dokumentasjonsverktøy har vi valgt å bruke \LaTeX{}. 
%TODO :: begrunnelse på alle verktøyvalg, nevne alternativene som ble forkastet..

% 
%VRML -- Dette vil si at man for det meste deklarer noder, og gir den forskjellige attributter med verdier som forteller hvor de respektive nodene skal være på skjermen. Det elementære i VRML gikk dermed fort å lære, men vi har brukt lengre tid på å få til de mer avanserte delene av VRML, som ruter, interpolatorer og trykksensorer. VRML har også en del avanserte felter som vi måtte lære hva gjorde og hvordan bruke dem, og disse har en annerledes syntaks enn noe vi har vært borte i før. Vi brukte utviklingsmiljøet VRMLPad til å eksperimentere og forske på disse, slik at vi kunne tilegne oss nok kunnskap til å senere bruke disse feltene.
\subsection{Faglige forutsetninger}

Vi har trengt å tilegne oss ny kunnskap om språkene og modulene som brukes i prosjektet, og lære oss ny syntaks raskt. Tidligere programmeringsfag har gitt oss erfaring med flere typer språk, og gitt oss god forståelse for programmering generelt. Dette grunnlaget har hjulpet oss å ta i bruk Perl, VRML og de respektive modulene.
%Sett bort i fra de rene programmeringsfagene, er det to fag som har bedret vår forståelse av komponentene i profilfilene;
% Våre XML-profiler kan inneholde flere hundre tjenester og konfigurasjoner i form av komponenter, og de to fagene nevnt har lært oss mye akkurat dette. 
Fagene `Operativsystemer og Unix' og `Nettverks- og Systemadministrasjon' har gitt oss innblikk i Perl-programmering og administrasjon av Unix-tjenester. Dette har hjulpet oss i programmeringen, samt til å forstå mange av konfigurasjonsparametrene som forekommer i profilene.Dermed har det blitt lettere å velge ut komponenter til visualisering. 
For eksempel forteller noen av profilene at en maskin er Apache- eller PostgreSQL-servere, og uten fagene hadde vi hatt liten kunnskap om hva dette sto for.

Relasjonsdatabasefaget har gitt oss grunnleggende kunnskap om databaser. For å komme fram til bedre algoritmer for vektor- og posisjonsberegning har fagene `Lineær Algebra' og `Algoritmer og datastrukturer' hjulpet oss. I systemutviklingsfaget har vi lært om forskjellige systemutviklingsmetodikker, som har gitt oss bedre forståelse for planlegging og strukturering av prosjektet.
%Normalisering av tabeller, og avanserte spørringer til en MySQL-database er viktig og relevant i forhold til vårt prosjekt, og det er noe av det vi lærte i dette faget.

\subsection{Hva måtte vi lære oss}
%[TODO: Fjerne dette avsnittet?]
Gruppa hadde på forhånd svært liten erfaring med visualisering generelt, og ingen erfaring med 3D-modellering. Derfor måtte vi bruke mye tid på å lære VRML og 3D-tankegang, samt få en bedre forståelse av visualiseringsteknikker og utnytte dette i prosjektet. Vi har heller ikke brukt \LaTeX  som dokumentasjonsverktøy før, og ser på prosjektperioden som en god anledning til å lære oss dette.
\subsubsection{VRML og tredimensjonal geometri}
**TODO: Finne bedre tittel på avsnittet
Koordinatsystemer og vektorer
En 'verden' i VRML har en struktur der alle objekter er plassert i et globalt koordinatsystem. I tillegg kan det defineres transformasjonsobjekter, som danner lokale koordinatsystemer for de objektene som tilhører dette. 
Den globale posisjonen til et objekt kan dermed avhenge av plasseringen i flere lokale koordinatsystemer.

Muligheten til å bruke lokale referanser gjør det enklere å plassere grupper av objekter i forhold til hverandre siden man først kan plassere objekter som hører sammen i forhold til hverandre for deretter å plassere gruppen av objekter globalt.

Et eksempel er laget i figur \ref{eksempel}.

\begin{figure}[ht]
\centering
\includegraphics[scale=0.5]{lokale_globale_koordinater_uten_tekst.PNG} 
\caption{VRML-koordinater} \label{eksempel}
\end{figure}

Her er trekanten plassert i posisjon (0,0,0) globalt. 
Boksen er satt i (-10, 5, 0), og ballen og sylinderen er en egen gruppe som er plassert i posisjon ( 10, -5, 0). Sylinderen har i tillegg en lokal plassering (0,-2,0) inne i denne gruppen.

\subsubsection{Visualiseringsteknikker}
En oppgave vi hadde var å prøve ut ulike visualiseringsteknikker, for å se hvilke som kunne passe til forskjellige data.

Resultatet vil bli en visualisering av forskjellige grupper ( clustere ) av datamaskiner, heretter kalt noder, der nodenes konfigurasjons(u)likhet kommer klart frem, for eksempel gjennom nodenes form, farge og posisjoner i et tredimensjonalt rom.

For å få litt inspirasjon, pekte vår veileder oss til en masteroppgave \cite{masterVis} der det var presentert mange forskjellige måter å visualisere data på. 
Noen teknikker vi ønsket å prøve var blant annet:

%TODO: Sette inn illustrasjoner på de ulike teknikkene.
\paragraph{Information pyramids} (fig. \ref{pyramid} )
Her vises informasjon i flere lag som et hierarki. 
Eksempelvis kan man tegne opp et lag underst som representerer alle noder (A), trinn 2 består av noder fra A som oppfyller et bestemt kriterie B, slik at  
Trinn 2 $ = A \cap B $.
%TODO: Heller bruke | Antall | i formlene? 
%og neste lag nok kriterie (C):  $Trinn 3  = A \cap B \cap C $.
Det er da mulig å få et inntrykk av hvor mange noder som oppfyller forskjellige kriterier.
\begin{figure}[ht]
\centering
\includegraphics[scale=0.5]{pyramid.PNG} 
\caption{Information pyramid} \label{pyramid}
\end{figure}

\paragraph{Scatter plot}
Dette består av noder som er spredt rundt i et område. 
Posisjonen til en node kan fortelle noe om nodens egenskaper, litt som en graf.
Et eksempel vises i figur \ref{scatterplot}.
\begin{figure}[ht]
\centering
\includegraphics[scale=0.5]{Scatterplot2.PNG}
\caption{scatterplot} \label{scatterplot}
\end{figure}
\paragraph{Heatmaps}
Ved å legge noder i et plan, og deretter fargelegge de delene som oppfyller et kriterie, vil man kunne få noe som minner om et kart, der interessante områder blir uthevet. Eksempelet i figur \ref{heatmap} viser gjennomsnittlig kvadratmeterpris i forskjellige bydeler i New York \cite{heatmapEks} .
\begin{figure}[ht]
\centering
\includegraphics[scale=0.5]{heatmap.jpg} 
\caption{Heatmap} \label{heatmap}
\end{figure}



\paragraph{Tree visualization}
Kan vise nodens indre struktur, mulig å sammenligne to eller flere trær.
Jfr. nodenes struktur, vil dette ikke være et typisk binærtre.
En mulighet vil også være å forsøke å visualisere en eller flere "standard-noder" basert på statistisk analyse av dataene, og så sammenligne enkeltnoder opp mot standarden.

Vi ønsker å forsøke å kombinere flere av disse teknikkene der dette er mulig, for å kunne trekke ut informasjon og sammenhenger som er vanskelig å finne ut av ellers.




\subsection{Hva skal vi visualisere?}
For det første trengte vi å få oversikt over dataene, for å se hva de representerte. Etter hvert ble det klart at for å avgrense oppgaven, begrenset vi oss til components-seksjonen i filene.
Til å hjelpe oss å analysere, laget vi et perl-script som ved hjelp av libXML gikk gjennom alle profil-filene, og fant antallet forskjellige komponenter, samt hvor mange maskiner som hadde disse.
Dette gjorde det enklere å få oversikt over hvilke komponenter som var i bruk.
Etter at vi hadde lest gjennom dokumentasjonen over komponentene (link?) og sett hvilke parametere som var vanlige, satte vi opp en liste over hvilke aktuelle felter i samarbeid med oppdragsgiver / veileder.
Eksempler av komponenter vi satt opp:
\begin{itemize}
\item[$\triangleright$] inv / os -$>$ Operativsystem
\item[$\triangleright$] inv / location -$>$ Fysisk lokasjon
\item[$\triangleright$] apache -$>$ Betyr at maskinen kjører en webservertjeneste.
\item[$\triangleright$] network / gateway -$>$ default gateway for maskinen.
\end{itemize}

Siden oppgaven var relativt løst formulert, hadde vi vanskeligheter med å slå oss til ro med de komponentene vi hadde valgt. Derfor konsentrerte vi oss heller på at visualiseringene skulle bli såpass generiske at de heller tar i mot parametre i form av komponenter, som da blir visualisert i VRML generatoren.
 
%DBI (Database Interface) er et databasegrensesnitt for Perl, som vi valgte å bruke for å lage og eksekvere spørringer til en database. Vi hadde liten forkunnskap om modulen, men syntaksen var lett å skjønne.  DBI-modulen kan koble seg opp mot mange databasemotorer på en enkel måte, og spørringer og svar er enkelt å fremstille og hente.

\section{Utviklingsprosessen}

\subsection{Metodikk / prosessmodell}
Gruppen bestemte seg tidlig for å benytte seg av en smidig utviklingsprosess, med mange iterasjoner og konstant utvikling av kravspesifikasjonene.
Vi valgte å bruke RUP (Rational Unified Process) som prosessmodell, med innslag av XP (eXtreme Programming).
XP-elementer ble valgt blant annet fordi vi måtte komme raskt i gang med programmeringen, for å gjøre oss kjent med språkene, vi synes også parprogrammering kan være gunstig innimellom.
\textbf{Testdrevet utvikling...?}

Måten vi jobbet på for hver visualisering: Til å begynne med laget vi en prototyp kun med VRML-editor på hva vi ønsket oss.
Så gikk vi over til å lage et perl-script der vi laget metoder for å generere ulike vrml-elementer og skrev så resultatet fra kjøringen ut til fil.
Deretter inspiserte vi den genererte koden og sjekket at den virket i en vrml-viewer. Ved syntax-feil rettet vi opp vrml'en og la evt. til kode manuelt, testet dette og gikk så tilbake til scriptet og endret / la til metoder her som så genererte ny vrml-kode.

\subsection{Faser}
Fasene i prosjektet fulgte RUP-fasen, med mange iterasjoner for å forbedre og utvikle koden.\\
Funksjonaliteten ble utvidet gjennom en inkrementell prosess, der vi startet med kun en visualiseringsteknikk og gjorde denne ferdig, før vi startet på en ny.

Innledning\\
Utforming\\
Bygging\\
Overgang





\subsection{Oppbygging av programmet}
Programmet består av to hoveddeler: dataImport og dataVisualiserer.
(fig. \ref{systemskisse})

Vi har valgt å implementere en 3-lags struktur med DAL ( Data Access Layer) for database-tilkobling, BLL (Business Logic Layer) for generering av VRML, og GUI (Graphical User Interface)  for kommunikasjon / presentasjon for sluttbruker.
Dette er gjort for å gjøre systemet utvidbart og generisk - eksempelvis vil det være mulig å gå over til en annen databasemotor ved kun å endre / bytte ut DAL.
Videre har vi valgt å implementere løsningen som en Web-applikasjon, da kombinasjonen *AMP (Apache, MySQL, Perl) er en god og plattformuavhengig kombinasjon, samt at det ikke stiller store krav til klienten, som egentlig kun trenger å kunne vise HTML, javascript og VRML.
\begin{figure}[ht]
\centering
\caption{Skisse av systemets oppbygging}
\label{systemskisse}
\includegraphics[height=8cm]{systemskisse.JPG} 
\end{figure}
\newpage
\noindent
Også DAL og BLL er delt opp i mindre moduler for å gjøre det enkelt å legge til funksjonalitet. DataImport-delen er ansvarlig for å lese inn nye datasett i form av XML-filer, og ekstrahere de feltene vi ønsker å trekke ut, for så å legge disse inn i database. %Denne delen er så delt opp i moduler for hver komponent som skal trekkes ut, dette for å gjøre det enkelt å utvide importen til å gjelde flere komponenter / felter. Forhåpentligvis gjør dette også det lettere å endre konfigurasjonen til å kunne importere andre XML-filer, dersom andre senere vil importere andre typer filer.
\subsection{Valg av teknologier}
\subsubsection{XML-tolker} %TODO: Putte dette et skikkelig sted
For å tolke XML-filene, trenger vi moduler som kan hjelpe Perl til å skjønne XML-struktur slik at vi får de dataene vi vil ha. Vi valgte først å bruke DOM (Document Object Model) til å tolke disse filene. Når vi ble godt kjent med syntaksen lagde vi et testskript i Perl, som vi testet på et lite sett med filer. Da vi senere skulle parse alle filene, viste DOM seg å være veldig treg, og minneforbruket ble så stort at våre maskiner fryste seg. 
Vi løste midlertidig dette problemet ved å kalle en \verb " doc->dispose() " metode for hver fil vi hadde lest inn, fordi Perls Garbage Collector ikke selv gjorde dette. Men fortsatt var vi ikke fornøyd med hastigheten. Vi søkte etter en ny modul, og fant LibXML som innfridde de forventninger vi har til en XML-parser. Dette er en XML parser til Gnome biblioteket (et Unix-grensenitt), og viste seg å være utrolig kjapp. Syntaksen på XML-spørringene er litt annerledes fra DOM, så noe omskriving måtte til. Som nevnt trenger vi å parse et stort antall XML-filer og LibXML viste seg å være raskere og mer effektiv enn DOM. Vi testet de to forskjellige modulene på vårt datasett, og det viste seg at LibXML er ca ti ganger kjappere, og brukte ca. en prosent av systemminne, mens DOM forsynte seg av rundt 70 - 80 prosent. 
\subsubsection{Import til database}
Vi ville få til import til databasen så generisk som mulig, og fant ut at en konfigurasjonsfil vil erstatte våre dataImport-moduler på en god måte. Modulene vi tenkte oss i første omgang gir oss en god mulighet for å ekstrahere data, men det ville bli en for stor jobb for en potensiell systemadministrator og skreddersy perl-moduler etter behov. Konfigurasjonsfilen er tenkt slik at den vil ta i mot en kriterier fra en web-applikasjon, for eksempel databasetilkoblingsinformasjon, lokasjon og namespace av XML-filene, og hvilke komponenter som systemadministratoren senere vil visualisere. Perl-scriptet XML\_to\_DB leser inn informasjon fra konfigurasjonsfilene, lokaliserer dem, oppretter databasetilkobling og ekstraherer ut ønsket komponentinformasjon fra filene og legger disse inn i databasen. I DBMETODER-modulen ble det opprettet to nye metoder, én for å opprette en ny tabell og én for å injiserer nye verdier inn i databasen, og disse er begge generiske. 
TODO: Kjøre tester som sammenligner de generiske metodene mot de som ikke var det. Bør ta lengre tid for de generiske.
De generiske tabellene som blir opprettet er ikke normalisert til høyeste nivå med hensikt. Med tanke på at vi senere ville få nye datasett, måtte vi sette opp en ny datastruktur i databasen. I hver eneste tabell vil det være et kolonnenavn `sist\_oppdatert', som sammen med maskinnavn vil danne primærnøkkelen for de respektive radene. Alle XML-filene har et obligatorisk element kalt `last\_modified' , og denne vil vi bruke for å sjekke om filen har grunnlag for å muligens bli importert til databasen eller ikke. 
Når vi importerer data til databasen, er vi som oftest kun ute etter noen få komponenter. Selvom `last\_modified' er oppdatert siden sist, betyr ikke det nødvendigvis at de komponenter som det er ønske for skal bli visualisert har blitt forandret. Derfor måtte det også implementeres en kryssjekk, som ser om det er ren redundant data som vil bli injisert. Hvis det er blitt noen forandringer, vil det opprettes en ny rad med nye data.
Etter som vi har blitt mer kjent med datasettene (XML-filene) og vi har fått et klarere bilde over hvordan databasen kommer til å se ut, fikk vi nye datasett fra arbeidsgiver. Disse nye datasettene var over et tidsløp på nesten 3 måneder, og vi trengte nå å oppdatere våre metoder for å injisere nye verdier til databasen. 
Tidligere har vi brukt databasemetoder i DBMETODER-modulen, og sendt med verdier til den for at den skal gi omdanne disse til SQL-spørringer. I vårt tilfelle har vi litt over 1000 XML-filer som vi vil ha inn i databasen, og til nå er det to hovedkomponenter vi bruker fra databasen til visualisering (inv og network). DBMETODER er bygd opp slik at den åpner en ny databasetilkobling for hver eneste spørring, og det resulterte i at ved kjøring av XML\_to\_DB ble det gjort omtrent 2000 databasetilkoblinger under kjøring på vår filbase. XML\_to\_DB bruker totalt 156 sekunder på å legge inn data til tre tabeller fra 1047 filer. Vi vil redusere denne tiden, og vil prøve oss på å objektorientere DBMETODER-modulen slik at vi har en databasetilkobling som vil være åpen fra scriptets start til slutt. Vi kaller denne nye modulen xtd.pm, og kopierer over de generiske databasemetodene fra DBMETODER-modulen.
Det ble gjort tre sammenligninger, én hvor vi satt inn et helt nytt datasett (1047 filer) inn i en ren database med én komponent, én hvor vi satt inn et helt nytt datasett (1047 filer) inn i en ren database med tre komponenter, og én hvor vi la inn et nyere datasett over et gammelt. Det viste seg på de to første testene at tidsdifferansen på sammenligningene var under to sekunder, og det så ut til at det ikke spilte noen rolle om databasetilkoblingen ble åpnet for hver spørring eller om den var åpen hele tiden. 
På den tredje testen derimot, var det en signifikant tidsdifferanse. 
I denne testen ble det også sjekket for redundant data, for hver eneste injisering måtte man først kryssjekke om det var noe nytt som skulle inn i databasen. Ved hjelp av DBMETODER-modulen ble det brukt 71.98 sekunder. Ved hjelp av xtd.pm ble det brukt 42.05 sekunder. Da er det tre faktorer som spiller inn for at vi har valgt å bruke en objektorientert modul til importering av data: Det er penere og ryddigere kode, det er kjappere og x. Når dette systemet er i gang, vil scriptet bli brukt mest til å importere et nytt datasett over et annet og derfor er en OO-løsning mest å foretrekke her.

Noen komponentverdier i XML-filene inneholder tegn som ødelegger SQL-spørringene i xtd-modulen, og det har vi måttet vise hensyn til. Ved at vi har prøvd nå til sammen rundt 80 sett med XML-filer (rundt 80 000 filer) inn til databasen er det to tegn som kan gå igjen, ' og ; . Løsningen til nå er å bruke substitusjon og regulære uttrykk i xtd-modulen, og fjerne disse tegnene. Forhåpentligvis vil vi senere utvikle en bedre metode for dette, som vil sjekke for andre ødeleggende tegn slik at vi unngår SQL-injeksjon.

\subsubsection{Databasestruktur}
Vi har valgt å bruke mySQL (My Structured Query Language) som databasemotor.  Dette fordi vi er familiære med syntaksen, den er plattformuavhengig og tilgjengelig som åpen kildekode.
%Vi har valgt å benytte mySQL som database.
Etter å ha analysert XML-filene, kom vi fram til at det ville være hensiktsmessig å opprette en tabell for hver konfigurasjonsdel som vi ønsker å importere.
Et eksempel: i XML-profile-filene har vi som oftest en konfigurasjonsdel $ <network> $.
Barnenoder av network er for eksempel $ <gateway> $, så dersom vi ønsker å importere denne taggen, oppretter vi en tabell network, med et felt gateway.
Som standard opprettes alle felt som VARCHAR(200).
Primærnøkkelen i disse komponent-tabellene vil være maskinnavn kombinert med datoen spesifisert i filens $ <last\_modified>$-tag, da konfigurasjonen kan endres over tid og vi ønsker å ta vare på endringer i konfigurasjon.
Ettersom det skal være mulig å legge til nye datafelter til import, vil databasen vår bli utvidet med flere tabeller eller felter etter behov. 
Databasen vår er ikke designet med tanke på høyeste grad av normalisering, da vi valgte å fokusere på andre oppgaver, men det ville vært hensiktsmessig å opprette en mastertabell over maskiner, last\_modified og en index som kunne brukes som fremmednøkkel i de øvrige tabellene.
Vi diskuterte også muligheten for å opprette en tabell over hvilke komponenter som ble importert, samt en tabell over forandringer. 
Dette ville mest sannsynlig resultert i en bedre og mere robust databasestruktur.
Et eksempel på E/R-diagram vises i figur ...* sett inn bilde her.. 
%I noen tilfeller vil denne strukturen ikke være gunstig, slik at noen tabeller vil være bygget opp på en annen måte. TODO: sette inn eksempel på dette...
%Vi ønsker også at systemet skal være utvidbart, så det bør være relativt enkelt for sluttbruker å velge nye felter i XML-profilene som utvider eksisterende tabeller, eller oppretter nye dersom tabellen ikke eksistererer.
%**TODO: Sette inn ER-diagram.***
%end putte dette et annet sted
\subsection{Datavisualizer}
dataVisualiserer-delen har ansvaret for å hente data fra databasen og generere VRML som så blir presentert for brukeren. 

\subsubsection*{Beregning av 3D-posisjoner}
En viktig del av visualiseringen har vært å finne algoritmer som kan brukes til beregning av posisjoner i det tredimensjonale rommet. Siden posisjonene i VRML er oppgitt i kartesiske koordinater, har det vært hensiktsmessig å benytte vektorer for å beregne plasseringen av objektene. 

I visualiseringen har vi funnet det nødvendig å ha to metoder for å generere vilkårlige koordinater. Den ene genererer en vilkårlig posisjon innenfor en boks med gitte dimensjoner, og den andre genererer en posisjon mellom to sfærer som begge har sentrum i origo. Begge metodene returnerer en array på tre elementer som representerer en tredimensjonal vektor.\\
Metoden for beregning av posisjon innenfor en boks returnerer en array med villkårlige verdier mellom null og angitt maksimalverdi for henholdsvis bredde, høyde og dybde.\\
\\
Metoden for beregning av koordinater mellom to sfærer er litt mer komplisert. Den trenger to parametere som angir radius på den indre og den ytre sfæren. I tillegg tar den en parameter som angir hvilken avstand som kan brukes til å gi en skalering av avstanden mellom to posisjoner. Metoden genererer først en tilfeldig vektorlengde som ligger mellom de to grensene angitt ved de to første parametrene delt på avstandsparameteren. Denne vektorlengden representerer radius r i likningen for en kule med senter i origo der x, y og z er aksene: \newline
\begin{equation} r^{2} = x^{2} + y^{2} + z^{2}\end{equation}
X-verdien settes først til et tilfeldig tall slik at $ x \in [ \: 0,  r \: ] $. Deretter settes y tilfeldig slik at
$y \in [ \: 0,  \sqrt{ r^{2} - x^{2} } \: ] $
før z-verdien til slutt beregnes ut fra r, x og y:
\begin{equation}z = \sqrt{ r^{2} - (x^{2} + y^2) }\end{equation}
De tre beregnede verdiene for henholdsvis x, y og z er alle positive flyttall. De representerer derfor kun punkter i første kvadrant. For å få negative verdier, og dermed kunne fylle en hel sfære, blir hver av verdiene tilfelig multiplisert med 1 eller -1. I tillegg multipliseres hver av verdiene med avstandsparameteren for å få tilbake ønsket skalering, og verdiene konverters til heltall før de returneres som en array på tre elementer.


\subsubsection{Generering av farger} 
VRML krever at fargene skrives som en tredimensjonal vektor, der hvert tall i vektoren representerer fargene rød, grønn og blå. Disse trenger å ha verdier mellom 0 og 1. 
Dette skal omskrives til generering av farger.. 
17/4-08
Vi ønsker å generere X antall farger, 
der X er de forskjellige verdiene til et kriterie.

I VRML kan fargene skrives som en tredimensjonal vektor der rød, grønn og blå komponentene har verdier mellom 0 og 1.

Utfordringer:
Menneskeøyet er ikke spesielt godt egnet til å se forskjell på farger, 
**link** ? eller skrive om setn. 
slik at vi bør tilstrebe at 
fargene som brukes er nokså forskjellige.
Dette blir naturligvis vanskeligere dersom vi har svært mange forskjellige kriterier.

 En løsning: Filtrere ut informasjon -- ta bort noen grupper når man studerer visualiseringen, for å ikke ta feil. Vi bør også sette en max-grense for antall farger -- 
 Over et visst punkt er det ikke lenger praktisk mulig å se forskjell på fargene, og da er det ikke hensiktsmessig å bruke farger på kriteriet. Ca 20 - 30 stk???

Da vi har svart bakgrunn, 
kan vi heller ikke bruke veldig mørke toner, 
fordi disse ikke er tydelige nok.

Første utkast : Satte statiske farger og brukte de.

Deretter: Generering av farger:
Bruke tre nestede for-løkker, øke en komponent med "step" 0.2...

Regnbue -- generere roggbif -- deretter øke en "ting" og legge til ,gjenta loop.

TODO: sette inn algoritmen for fargevalg via vektorer og dens lengde...

\subsubsection{visualisering i praksis} %Evt. programmering... 
Det første vi prøvde å visualisere var maskinnoder og hvilket os de har.
For at vår visualisering skulle passe til forskjellig antall maskiner, telte vi opp det totale antall maskiner, m.
Ved å ta kvadratroten av dette: $ n = \sqrt{m} $ kunne vi lage en grid med n * n noder.
Størrelsen på nodene ble av praktiske hensyn satt til 1 * 1 * 1 , og hver node ble satt inn med et mellomrom på 1.
Dette gjorde at vi fikk en formel for høyde og bredde på denne visualiseringen, basert på antall maskiner m:   $ h = w = 2 * \sqrt{m} $
Dette trengte vi for å kunne beregne posisjonen for et viewpoint, altså hvor "kameraet" skal stå for å kunne vise hele scenen.
Et viewpoint i VRML har som standard et synsfelt på $ \pi/4 $ grader, men kan settes fra alt til 
0 - 2 $ \pi $.
Vi valgte å beholde standard, blant annet for å unngå "distortion" --  dersom man øker eller minker for mye vil man få sammentrekninger eller "fisheye"-effekt.

Med en vinkel på 90 grader var det enkelt å beregne hvor kameraet skulle stå.
(Vi begynner i punkt 0,0,0).
TODO: Illustrasjon???
Med en høyde h og bredde b, kunne vi bruke trigonometri til å beregne kameraets x,y, og z posisjoner.
x og y settes til hhv b / 2 og h /2.
z kan regnes ut ved å beregne $ x / \tan 45 = x $  og for å få litt luft på hver side setter vi z til 3 * x. Dette resulterte i en visualisering som den i fig.: **illustrasjon?**

Neste oppgave ble nå å lage en annen visualiseringsmetode for et kriterie. 
Vi valgte å ta utgangspunkt i maskinens gateway, fordi det var et felt som nokså mange maskiner hadde, og det var ikke så altfor mange forskjellige.
For å vise tilhørighet, bestemte vi oss for å la maskinnodenes posisjon si noe om hvilken gateway de tilhørte. Sagt tydeligere: Vi tegnet opp gateways og posisjonerte nodene i nærheten av sin respektive gateway.
Ved å ta utgangspunkt i det første scriptet, endret vi det til å hente inn de forskjellige gateway-addressene.
Så lagde vi en grid basert på dette antallet og genererte en kule for hver gateway.
Deretter hentet vi inn alle maskinnavn sortert på tilhørende gateway fra databasen.
Nå kunne vi løpe gjennom listen over maskiner og tegne en firkant for hver maskin, og sette posisjonen til området rundt den respektive gatewayen.

Da dette virket, endret vi det til å sette en tilfeldig posisjon for hver maskin, og deretter flytte det mot gatewayen som en animasjon, både for en penere effekt, men også fordi vi ønsket å prøve ut bevegelse i visualisering, da dette er noe menneskeøyet oppfatter spesielt godt.
Så ved å prøve ut dette her, kunne vi bruke metodene senere i andre visualiseringer.
For å få til dette, måtte vi opprette grupper ( transforms ) for hver gateway.
Denne gruppen fikk så en posisjonsbeskrivelse med start og slutt-koordinater, hvor sluttkoordinatene er de samme som den tilhørende gateway-nodens koordinater. 
Alle maskinnoder som hadde denne gatewayen ble lagt inn i denne gruppen på et tilfeldig sted.
En animasjon er bevegelse over tid, så vi måtte også lage en klokke (timer), og rute klokkens "tikk" til posisjonsbeskrivelsen. 
Så må denne rutes videre til sin respektive gruppe, som så flytter alle nodene i gruppen mot gatewayen. Maskinnodene selv ble også tildelt en lokal. tilfeldig posisjon rundt gatewayen, så ikke alle skulle bli liggende i en klump.

Når også dette var i orden, kombinerte vi os-visualisering og gateways, slik at nodene fikk farge etter hvilket os de hadde og ble posisjonert etter hvilken gateway de tilhørte, se eksempel figur \ref{gv}.
\begin{figure}[ht]
\centering
\includegraphics[scale=0.5]{groupvisualisation.PNG} 
\caption{Group Visualisation} \label{gv}
\end{figure}

Denne visualiseringen har vi valgt å kalle en ``gruppe-visualisering med to kriterier'', fordi noder blir gruppert etter kriterier. 

Etter dette ønsket vi også å ha mulighet til å filtrere bort uønsket informasjon, for eksempel ved at man kan slå visning av grupper av operativsystem av og på.
På dette tidspunktet var ikke gruppene organisert på annen måte enn gateway, så for å få til å velge alle noder med et bestemt operativsystem, ble det nødvendig å organisere nodene annerledes.

Løsningen ble å lage nøstede grupper av maskiner, slik at vi opprettet en gruppe for hvert operativsystem. Inne i denne gruppen opprettet vi så nye grupper over forskjellige gateways, og inne i disse ble maskinene plassert.
Det er da en smal sak å få manipulert en gruppe med maskiner, og vi kan da filtrere ut maskiner basert på hvilket operativsystem de har.

Datastrukturen som ble brukt, var en hash med hasher, altså på formen:

\begin{verbatim}
	%nodes{ 
		%os1{
			machineName1 => gwAdr1,
			machineName2 => gwAdr2		  	
		}
		
		%os2{
			machineName3 => gwAdr1,
			machineName4 => gwAdr2		  	
		}			
	}
\end{verbatim}

Ved å sortere hashen på først nøkkel 1, og deretter på verdien (gwAdr), kunne vi løpe gjennom datastrukturen og generere VRML-grupper for hver unike verdi, og så plassere nodene inne i disse.

Da dette var i orden, ønsket vi å kombinere enda mer informasjon.
Alle nodene som oppfyller et boolsk kriterie ble hentet og lagt inn i en ekstra datastruktur. Så, da hver node printes, sjekket vi om vi fant nodenavnet i kriterie3. Hvis ja, la vi til en rute som trigget en animasjon på noden.
Vi testet ut flere forskjellige teknikker, for å se hva som var best. 
Til slutt endte vi opp med å sette en rotasjon på noden, slik at den spinner rundt sin egen akse, fordi dette ikke genererte for mye "visuell støy". 
Noen andre forsøk var å flytte de fram og tilbake i z-planet, eller skalere nodene opp og ned, men begge disse ble forkastet fordi vi syntes de tiltrakk seg for mye oppmerksomhet.

Heatmap / Changespiral
Vi ønsket også å lage en visualisering som kunne vise forandring i nettverket over tid. 

Det første forsøket var å generere et objekt per cluster, der objektets størrelse tilsvarer antall maskiner som oppfyller kriteriet.
Ved å hente ut alle maskiner fra databasen, sortert etter dato, kunne vi få en oversikt over størrelsen på enhver gruppe til enhver tid.
Deretter ble det opprettet en klokke, som for hver nye dato, skalerte størrelsen til gruppene etter hvor mange maskiner som ble lagt til / fjernet.
Dette fungerte fint, men svært store grupper ble veldig dominerende, slik at i stedet for å sette forholdet til 1: 1, valgte vi å prøve å sette størrelsen til  tredjeroten av antallet (n). $ x = \sqrt[3] n $ 
Dette ble bedre i forhold til at mindre grupper også ble synlige, men det var vanskelig å se forandringen i størrelse, slik at en middelvei med kvadratrot ble valgt som et minste felles multiplum.

Vi ville også forsøke å visualisere andre endringer i konfigurasjonen.
Valget falt på å basere oss på et heatmap, fordi dette var en av grunnteknikkene som vi ikke enda hadde implementert. 
Hver gang konfigurasjonen til en maskin blir oppdatert, legger vi dette inn i databasen. Så ved å hente ut alle maskiner fra databasen, sortert etter dato for endring / import, og så sjekke hva som endres: er det at maskinen går fra en gruppe til en annen, eller er det andre ting i konfigurasjonen som oppdateres?

Eks: Vi grupperer på OS. Vi begynner på den første datoen i databasen og henter ut alle maskiner som er lagt inn denne dagen. 
Datastrukturen er:
\begin{verbatim}
	\$groups{
				dato => os => antall
			}

	og \%changes{
				dato => os => antall
				}
\end{verbatim}

Vi lagrer også en hash med maskinnavn og tilhørende os, slik at om maskinens konfigurasjon blir oppdatert senere, kan vi sjekke om oppdateringen var OS eller noe annet.
Dersom det var "noe annet", øker vi changes med en.

Vi kan da se hvor mange maskiner i en os-gruppe som har oppdatert konfigurasjonen sin på et gitt tidspunkt. Dette antallet, delt på antall maskiner i gruppa totalt, gir et forholdstall mellom 0 og 1.
\begin{verbatim}
Endring i prosent per gruppe = \$changes{dato}{os} / \$groups{dato}{os} 
\end{verbatim}
En fargeInterpolator kan ta et tall fra 0 til 1 og returnere en farge.
Rekkefølgen på fargene har vi valgt å være primærfarger i et vanlig regnbuespekter fra blå til rød / fiolett. 
Ved å bruke dette tallet og rute inn til fargeinterpolatoren, kan gruppefargen settes til en farge som sier noe om hvor mange forandringer en gruppe gjennomgår, dersom ingen endring skjer er fargen blå, og dersom svært mange forandringer inntreffer, vil fargen gradvis gå over til rød / fiolett.

For å få posisjonert gruppene hensiktsmessig, forsøkte vi flere teknikker, blant annet en matrise. Men vi syntes det ble penere å organisere de i en sirkulær form, der vi plasserte de største gruppene innerst, og økte radiusen etter hvert, slik at det dannes en spiralform.
\begin{figure}[ht]
\centering
\includegraphics[scale=0.7]{spiral_manager.PNG} 
\caption{Spiral heatmap} \label{spiral}
\end{figure}


\subsubsection{GUI}
Vi ble enige om at vi ville ha et brukergrensesnitt som er enkelt og funksjonabelt, hvor hovedfokuset skal være på visualiseringsfilen. Med en browser-basert løsning har vi muligheten til å gi et enkelt grensesnitt til bruker, og vrml fila kan legges inn og oppdateres kontinuerlig. Vi valgte å bruke en av Perl's moduler, CGI (Common Gateway Interface), til å lage dette. CGI kommuniserer med Perl og Apache på en god måte, og siden det er laget i Perl trenger vi ikke introdusere nye språk / teknologier i brukergrensesnittet.

Hendelsesforløpet ved å få fram en visualisering i GUI er tenkt slik (gruppevisualisering her):
\begin{itemize}
\item[$\triangleright$] Velge visualiseringsteknikk
\item[$\triangleright$] Velge hvor mange kriterier man har lyst til å legge grunnlag for
\item[$\triangleright$] Velge tabeller og kriterier
\item[$\triangleright$] VRML-filen blir lagt til, og kommer opp på siden
\end{itemize}

Det har oppstått et par utfordringer underveis. Et problem ved utvikling av førsteukastet til websiden er at vi må skrive koden med absolutte stier for at Apache skal forstå hvor de respektive filene ligger. Websiden trenger flere moduler som vi har skrevet, og disse krever igjen andre moduler vi har skrevet - og for å få de rette modulene implementert i websiden har vi måttet skrive inn absolutte stier. Dette håper vi senere vil la seg løse ved at hele systemet blir gitt i en virtuell maskin, slik at de absolutte stiene faktisk er absolutte hvor enn systemet kjøres fra. 

Selve VRML-filen (.wrl) som ble lagt til i websiden ble ikke oppdatert av browseren eller apache. Ved første kjøring av websiden på en ren Apache-server fungerte det å hente opp den VRML-fila som akkurat ble generert av GUI, men ved neste visualisering (med helt andre kriterier) hentet den fortsatt opp den første genererte VRML-fila. Ved rensking av cache og temporært minne håpet vi det ville la seg ordne, men samme problem oppsto fortsatt. For å løse dette, må vi lage nytt navn VRML-filen hver gang den blir generert. Dette er ingen god løsning, kun en workaround, og vi ser helst at den blir løst på en annen måte i neste utkast.

Etter mye testing, viser det seg at nevnte bug er nettleserrelatert. Ved omstart av nettleseren blir riktig VRML-fil lastet opp på websiden, og den blir riktig lastet opp hver gang man ønsker ny visualisering. 

Brukergrensesnittet ble i første omgang kun prøvd ut på en relativt kraftig maskin. Ved testing på andre maskiner, blir VRML-filen inne i nettleserne altfor tung. Derfor har vi valgt å kutte ut muligheten for å legge til VRML-filen i nettleseren, og heller la grensesnittet bli brukt til å velge kriterier og generere VRML-filen på den lokasjonen som er spesifisert i programmet. 


\subsection{Strukturen i gruppa}
 - miljø, arbeidsgiver, veileder



\subsection{Hva var ekstra vanskelig?}
Kall dette utfordringer
Farger...\\
database.. \\


\subsection{Kravspesifikasjon}
%Dette er sakset fra forprosjektrapporten. Vi må få litt bedre / nyere kravspekk her.
Hovedmålet med prosjektet er å ekstrahere data og visualisere grupper av maskiner basert på konfigurasjons(u)likhet.
Vårt system skal kunne:
\begin{itemize}
\item[$\triangleright$] Lese inn de dataene vi har spesifisert fra XML-profilene og legge inn i en database.
\item[$\triangleright$] Hente de dataene vi har spesifisert fra databasen.
\item[$\triangleright$] Visualisere data på en eller flere hensiktsmessige måter.
\end{itemize}

\begin{itemize}
\item[$\triangleright$] Systemet skal være tilgjengelig som åpen kildekode.
\item[$\triangleright$] Visualiseringen vil gjøres med VRML i kombinasjon med javascripts.
\item[$\triangleright$] Programmeringsspråk vil være Perl for ekstrahering av data og generering av VRML.
\item[$\triangleright$] Brukerdokumentasjon, kildekode samt deler av sluttdokumentasjonen må være tilgjengelig på engelsk.
\item[$\triangleright$] Det er ønskelig at systemet er plattformuavhengig. 
\item[$\triangleright$] Systemet bør være generisk og utvidbart.
\end{itemize}


\begin{itemize}
	\item[$\triangleright$] Hva har den betydd, og hvilken rolle har den spilt for utviklingen.\\
I begynnelsen hadde vi en svært løs kravspesifikasjon, blant annet basert på samtaler med veileder, Paul Anderson og en mail fra han. \ref{mail}.
%** Få med henvisning til appendix, mail fra Paul Anderson. **
Underveis i prosjektperioden la vi til nye krav og raffinerte de eksisterende kravene, i samarbeid med oppdragsgiver.
Dette ble gjort fordi vi trengte kunnskap om både verktøyene og konfigurasjonsfilene, for å kunne sette realistiske krav, som vi så forsøkte å innfri så raskt som mulig, før vi gikk en ny runde med videreutviklingen av kravene våre.

	\begin{itemize}
		\item[$\triangleright$] gruppa
		\item[$\triangleright$] produkt 
\end{itemize}
	\item[$\triangleright$] Endringer ....
\end{itemize}

\subsection{Resultat}
Tolking av resultatet.

\section{Avslutning}
test av kilde: \cite{vrmlSpec}
\begin{itemize}
	\item[$\triangleright$] Eget utbytte
	\item[$\triangleright$] Oppsummering
	\item[$\triangleright$] Konklusjoner
	\item[$\triangleright$] Refleksjoner - Hva kunne vært gjort annerledes
	\item[$\triangleright$] Hva kan det brukes til? Fremtid
	\item[$\triangleright$] Hva syns oppdragsgiver om produktet vårt?
	\item[$\triangleright$] Skal det brukes videre?
\end{itemize}
\bibliography{kilder} 
\bibliographystyle{plain} 

\section{Appendix}


\label{mail} mail fra Paul Anderson

\begin{verbatim}
I'm enclosing one XML "profile" which is the entire configuration spec
for one machine (my desktop in this case). We have about 1500 of these
for the whole of our real installation, and we could probably also
extract a few years worth of historical ones from the CVS.

As I said, this should provide a fairly explicit format for extracting
data which we could visualise, but there will probably be some wrinkles
in practice. In case you feel like looking it over, here are a few
random initial comments ...

(*) the <components> section is probably all you want to look at.

(*) This has a list of configuration components - eg. the first one
called "LPRng"

(*) Each component has a list of key/value pairs ("resources") - eg.
buildperms = /usr/sbin/build-print-perms

(*) You can throw away the "derivation" attribute - this tells you where
in the source it is generated from (for debugging)

(*) Some resource values are compound structures called "tag lists"
which are like ordered records
    In this case, the "name" attribute gives the "tag" (record name). Eg.
    conf.printcap = printcap_path=| -\$ /usr/bin/pcap-query
    conf.nonprintable = check_for_nonprintable@
    Etc ....

(*) These structures can be nested - the XML for this probably looks a
bit odd.

(*) You can throw away the "template" attribute - this is used for
serialising these structures and forming variable names.

(*) I suspect that these structures will cause the most trouble in
practice, because sometimes the tags are significant and their values
are used by the component. Other times, the tags are arbitrary and it is
only the values and the order which are significant. This is a
consequence of using a single data structure ("tag list") to represent
both lists and records. This is historical, and I'm not sure whether it
is still a good idea or not! In practice, we could probably create a
kind of schema file for each component type saying which things were
significant.

(*) I'd be very interested in trying to visualise how "different" the
machines are. The metric for this would presumably be something simple
to start with like the number of resources which were different between
two machines. Once we see what this looks like, we would probably want
to refine this, for example with weightings. We might even want to play
with this dynamically.

(*) I suspect that a large number of the resource values never vary at
all for machines at our site - it would be interesting to know what
proportion - but these can probably just be thrown away (once they have
been identified).

(*) The values are nearly all going to be strings. Even when they are
numeric, I don't think it will make any sense to treat them as numbers
when computing any kind of metric.

(*) Some resource - eg. conserver.servers specify the names of other
machines. It should be possible to identify these resources and use
(only) those to identify dependencies between clients and servers (for
different services). This might make another interesting visualisation!

-----------------

Paul fremhever spesiellt to ting han er interessert i å visualisere:

1) Dependencies - elements in the configuration for one machine
reference other machines - this forms a dependency network.

2) Clustering - it would be nice to be able to compare the configuration
resources for machines and group the machines by how "similar" their
resources were, and then visualise the resulting clusters.


----
\end{verbatim}

\end{document}


