\documentclass[12pt,a4paper,onecolumn]{article}
\usepackage[latin1]{inputenc}
\usepackage[norsk]{babel}
\usepackage{verbatim}
\usepackage{alltt}
\usepackage{graphicx}
\usepackage{babel, textcomp}
\usepackage{url}
\usepackage{enumerate}
%remove the following line if not using special symbols
\usepackage{amssymb}
\tolerance = 5000      % LaTeX er normalt streng når det gjelder
                      % linjebrytingen.
\hbadness = \tolerance % Vi vil være litt mildere, særlig fordi norsk
                      % har så
\pretolerance = 2000   % mange lange sammensatte ord.

\begin{document}
\title{Prosessrapport}
\author{Lars Martin Bredal \and Morten Byhring \and Tom Erik Iversen \and
Høgskolen i Oslo, avdeling for ingeniørutdanning}

\maketitle


\newpage 

\section{Forord}

Dette dokumentet forteller om planleggingen og arbeidsmetodene som er brukt i hovedprosjektet. 
Rapporten er optimalisert for papir, og det kreves noe kompetanse innen matematikk og informatikk for å forstå innholdet.
Vi ønsker å rette en stor takk til følgende personer som har hjulpet og veiledet oss i arbeidet med prosjektet:
\begin{description}
\item[Simen Hagen] - for konstruktiv kritikk og tilbakemeldinger 
\item[Paul Anderson] - for datamateriale
\end{description}

\newpage

\tableofcontents

\newpage
%TODO: Flytte tableofcontents ned til rett over hoveddel jfr dokumentasjonsstandard???

%\subsection{Sammendrag}
%Dette avsnittet vil bli brukt hvis vi føler det nødvendig å ta med et sammendrag av hele prosessrapporten når den er ferdig skrevet.


\section{Innledning}

\subsection{Om oppdragsgiver}
Oppdragsgiver er Høgskolen i Oslo, ved lektor Simen Hagen.
Simen har tidligere hatt kontakt med Paul Anderson, professor ved University of Edinburgh. De har diskutert muligheten for et prosjekt som går ut på å visualisere konfigurasjonsfiler.
%her kommer alt sakset fra forprosjektrapporten
\subsection{Dagens situasjon}
LCFG (Local ConFiGuration system) er et system som brukes for å administrere konfigurasjon av et stort antall UNIX-maskiner. Systemet er utviklet av blant annet Paul Anderson ved University of Edinburgh.
Konfigurasjonsinnstillingene distribueres ved hjelp av XML-filer (eXtendible Markup Language) som hentes fra en webserver. Hver maskin har sin XML-fil med en unik konfigurasjon.
Oppdragsgiver ønsker å få en grafisk oversikt over maskiner i nettverket, der disse er gruppert basert på hvor forskjellig maskinene er konfigurert. 
\subsection{Mål og rammebetingelser}

\subsubsection{Mål}
Hovedmålet med prosjektet er å ekstrahere data og visualisere grupper av maskiner basert på konfigurasjons(u)likhet.
Vårt system skal kunne:
\begin{itemize}
\item Lese inn spesifiserte data fra XML-filene og legge disse inn i en database.
\item Hente ønskede data fra databasen.
\item Visualisere data på en eller flere hensiktsmessige måter.
\end{itemize}

\subsubsection{Rammebetingelser}

Oppgaven er løst formulert fra oppdragsgivers side.
Det er ikke stilt spesifikke krav til hvilke data som skal visualiseres, eller hvilken visualiseringsmetode som skal brukes. 
Datamaterialet er svært stort, og det må beregnes at mye tid vil gå med til å få oversikt over dette.
På grunn av den store datamengden vil det i første omgang være naturlig å velge ut noen få verdier som brukes til visualisering, men systemet bør være skalerbart.
Vi har fått frie hender når det gjelder valg av utviklingsplattform og verktøy.
%end forprosjektrapport

%Det kan tenkes at innledningen på Hoveddelen ikke får noen tittel, slik som den har nå. Vi får se.

\section{Planlegging og metode}

\subsection{Datagrunnlag}
Datagrunnlaget for prosjektet er et sett XML-filer, der hver fil representerer konfigurasjonen til en maskin på et gitt tidspunkt. 
Første datasett bestod av 1060 XML-filer med en gjennomsnittlig størrelse på 1 MB.
Første fase av prosjektet gikk i stor grad ut på å få en oversikt over strukturen på disse filene og hvilke data som var tilgjengelige.
Oppdragsgiver hadde gitt en viss pekepinn på hvilke data som var mindre interessante, blant annet feilsøkingsinformasjon.
Dette ble fjernet, noe som halverte filstørrelsen og gjorde det lettere å lese filene manuelt.
%Disse filene kan endres over tid, slik at det er mulig å lage en historikk over endringer i konfigurasjonsprofilen dersom man tar vare på gamle data.
%
 %Vi startet med en samling på 1060 XML-filer der hver fil hadde en gjennomsnittlig størrelse på omkring 1MB. 
%Første skritt var å fjerne en del data som av oppdragsgiver var angitt som uinteressante for oss, da de kun var beregnet på feilsøking. 
%\\
%\\
%Vi kunne nå begynne å skaffe oss et bilde av datagrunnlaget ved å ta for oss en av filene og se på strukturen. Etter å ha fått en viss oversikt over strukturen kunne vi lage script for å hente ut den informasjonen vi ønsket fra alle filene for å skaffe oss en total oversikt over materialet. 
Neste trinn i prosessen var å velge ut hvilke data vi ville jobbe videre med. 
I dette arbeidet ble dokumentasjonen \cite{lcfgGuide} til LCFG brukt for å finne ut hva de enkelte verdiene representerte. Denne dokumentasjonen dekket ikke alle områder, men var likevel til stor hjelp. 
Det viste seg for eksempel at en del potensielt interessante datafelt hadde en annen betydning enn først antatt.\\

%\textbf{(eksempler?)}  Vi trodde Serverkomponenten betød at maskinen var en server, men den angir kun at den er en LCFG-server og sier ikke noe om andre tjenester.
%\\
%\\
Oppbyggingen til XML-filene kan oppsummeres slik:
Profilen består av to hovedseksjoner: \verb"components" og \verb"packages".
Seksjonen \verb!components! inneholder en rekke underseksjoner som representerer konfigurasjonen av et program, eller tjeneste (komponent), på maskinen.
En komponent kan ha både andre underseksjoner og bladnoder som inneholder informasjon.
Det er mer enn 100 forskjellige \verb'components' i datamaterialet.
Kun \verb'profile'-komponenten er obligatorisk i \verb!components!-seksjonen.
%TODO:
%Flere seksjoner:

\subsection{Verktøy}
Tidlig i prosjektprosessen hadde vi et møte med veileder hvor det ble diskutert hvilke teknologier som burde brukes til å gjennomføre prosjektet.
Det var nødvendig med et programmeringsspråk som raskt kunne tolke XML-data, kommunisere med en database og generere tekstfiler, i tillegg til
%er ekstra kjapt og hendig med XML-data (Extensible Markup Language), et som egner seg til å ekstrahere ut data fra en DB, et som egner seg til å generere filer 
 et språk som egner seg for å visualisere et stort antall noder i et tredimensjonalt rom. 
Da XML-filene er svært store, er det ønskelig å lagre informasjon i en database, både for å spare lagringsplass og tid ved henting av data.
Etter å ha lest om forskjellige teknologier og vurdert alternativene, falt valget på Perl og VRML (Virtual Reality Modeling Language) som programmeringsspråk og mySQL som databasemotor. 
%Etter råd fra veileder var det klart hvilke to språk som skulle være våre hovedverktøy; Perl og VRML (Virtual Reality Modeling Language). Perl er ypperlig til å innsette og ekstrahere data, og til å generere *.wrl-filer. Slik vi har tenkt oss å visualisere konfigurasjonen i LCFG, vil VRML fylle alle våre krav til framvisning av dette.
%Når vi startet prosjektet hadde gruppen noe erfaring med Perl. Bruksområdene til Perl i vårt prosjekt vil være ekstrahering av data fra XML og database, og generering av filer. Det vi tidligere har gjort i Perl , mens vi nå jobber mot XML, database og generering av filer. Siden vi alle på gruppen har jobbet litt med Perl, har det ikke vært noen stor jobb å tilegne seg mer kunnskap i disse områdene. %TODO : bør vi endre dette litt..?%
\begin{description}
\item[Perl] ble valgt på grunn av sin gode evne til å manipulere tekst. I tillegg er mye av LCFG skrevet i Perl, så gruppa mente dette var et naturlig valg.
\item[VRML] er et Markup-Language for 3D-modellering. Syntaksen ligner på HTML (HyperText Markup Language) og XML. Dette ble valgt blant annet fordi veileder har god kjennskap til dette språket.
\item[MySQL] (My Structured Query Language) er valgt som databasemotor fordi vi er kjent med syntaksen, den er plattformuavhengig og tilgjengelig som åpen kildekode.
\item[Eclipse,] med tilleggsmodulene EPIC (Eclipse Perl Integration) og Subclipse for versjonskontroll (Subversion) er brukt som utviklingsmiljø.
Dette ble valgt fordi det er et godt utviklingsmiljø til Perl, med integrert støtte for subversion.
\item[\LaTeX{}] er dokumentasjonsverktøyet brukt i prosjektperioden.
Begrunnelsen for dette var at det skulle bli lettere å fokusere kun på skriving, ikke formattering av tekst. Da \LaTeX{}-dokumenter kun består av ren tekst, er det også enklere å synkronisere dokumenter med Subversion.

\end{description}
%TODO :: begrunnelse på alle verktøyvalg, nevne alternativene som ble forkastet..

% 
%VRML -- Dette vil si at man for det meste deklarer noder, og gir den forskjellige attributter med verdier som forteller hvor de respektive nodene skal være på skjermen. Det elementære i VRML gikk dermed fort å lære, men vi har brukt lengre tid på å få til de mer avanserte delene av VRML, som ruter, interpolatorer og trykksensorer. VRML har også en del avanserte felter som vi måtte lære hva gjorde og hvordan bruke dem, og disse har en annerledes syntaks enn noe vi har vært borte i før. Vi brukte utviklingsmiljøet VRMLPad til å eksperimentere og forske på disse, slik at vi kunne tilegne oss nok kunnskap til å senere bruke disse feltene.
\subsection{Faglige forutsetninger}

Vi har hatt behov for å tilegne oss ny kunnskap om språkene og modulene som brukes i prosjektet, og lære oss ny syntaks raskt. Tidligere programmeringsfag har gitt oss erfaring med flere typer språk, og gitt oss god forståelse for programmering generelt. Dette grunnlaget har gjort det enklere å ta i bruk Perl, VRML og de respektive modulene.
%Sett bort i fra de rene programmeringsfagene, er det to fag som har bedret vår forståelse av komponentene i profilfilene;
% Våre XML-profiler kan inneholde flere hundre tjenester og konfigurasjoner i form av komponenter, og de to fagene nevnt har lært oss mye akkurat dette.
 
Fagene `Operativsystemer og Unix' og `Nettverks- og Systemadministrasjon' har gitt oss innblikk i Perl-programmering og administrasjon av Unix-tjenester. Dette har hjulpet oss i programmeringen, samt til å forstå mange av konfigurasjonsparametrene som forekommer i profilene. Dermed har det blitt lettere å velge ut komponenter til visualisering. 
For eksempel forteller noen av profilene at en maskin er Apache- eller PostgreSQL-server, og uten disse fagene hadde vi hatt liten kunnskap om dette.

Relasjonsdatabasefaget har gitt oss grunnleggende kunnskap om databaser. For å komme fram til bedre algoritmer for vektor- og posisjonsberegning har fagene `Lineær Algebra' og `Algoritmer og datastrukturer' vært til stor hjelp. I systemutviklingsfaget har vi lært om forskjellige systemutviklingsmetodikker, som har gitt oss bedre forståelse for planlegging og strukturering av prosjektet.
%Normalisering av tabeller, og avanserte spørringer til en MySQL-database er viktig og relevant i forhold til vårt prosjekt, og det er noe av det vi lærte i dette faget.

\subsection{Hva måtte vi lære}
%[TODO: Fjerne dette avsnittet?]
Gruppa hadde på forhånd svært liten erfaring med visualisering generelt, og ingen erfaring med 3D-modellering. Derfor måtte vi bruke mye tid på å lære VRML og 3D-tankegang, samt få en bedre forståelse av visualiseringsteknikker og utnytte dette i prosjektet. Vi har heller ikke brukt \LaTeX{} som dokumentasjonsverktøy tidligere, og ser på prosjektperioden som en god anledning til å lære oss dette.
\subsubsection{VRML og tredimensjonal geometri}
En ``verden'' i VRML har en struktur der alle objekter er plassert i et globalt koordinatsystem. I tillegg kan det defineres transformasjonsobjekter, som danner lokale koordinatsystemer for de objektene som tilhører dette. 
Den globale posisjonen til et objekt kan dermed avhenge av plasseringen i flere lokale koordinatsystemer.
Muligheten til å bruke lokale referanser gjør det enklere å plassere grupper av objekter i forhold til hverandre.
Objekter som hører sammen i forhold til hverandre kan grupperes, for deretter å plassere gruppen globalt.
Et eksempel er laget i figur \ref{eksempel}.
\begin{figure}[ht]
\centering
\includegraphics[scale=0.5]{lokale_globale_koordinater_uten_tekst.PNG} 
\caption{VRML-koordinater} \label{eksempel}
\end{figure}
Her er trekanten plassert i posisjon (0,0,0) globalt. 
Boksen er satt i (-10, 5, 0), og ballen og sylinderen er en egen gruppe som er plassert i posisjon ( 10, -5, 0). Sylinderen har i tillegg en lokal plassering (0,-2,0) inne i denne gruppen.

\subsubsection{Visualiseringsteknikker}
En av oppgavene vi hadde var å prøve ut ulike visualiseringsteknikker, for å se hvilke som kunne passe til forskjellige data.
Målet er å visualisere grupper (clustere) av datamaskiner, heretter kalt noder, der nodenes konfigurasjons(u)likhet kommer klart frem.
Dette kan for eksempel uttrykkes ved nodenes form, farge og posisjoner i et tredimensjonalt rom.
For å få litt inspirasjon, viste vår veileder en masteroppgave \cite{masterVis} som presenterte mange forskjellige måter å visualisere data på. 
Noen teknikker vi ønsket å prøve var blant annet:

%TODO: Sette inn illustrasjoner på de ulike teknikkene.
\paragraph{Informasjonspyramider}(fig. \ref{pyramid} )
Her vises informasjon i flere lag som et hierarki. 
Eksempelvis kan man tegne opp et lag nederst som representerer alle noder (A), trinn 2 består av noder fra A som oppfyller et bestemt kriterium B, slik at  
Trinn 2 $ = A \cap B $.
%TODO: Heller bruke | Antall | i formlene? 
%og neste lag nok kriterie (C):  $Trinn 3  = A \cap B \cap C $.
Det er da mulig å få et inntrykk av hvor mange noder som oppfyller forskjellige kriterier.
\begin{figure}[ht]
\centering
\includegraphics[scale=0.5]{pyramid.PNG} 
\caption{Informasjonspyramide} \label{pyramid}
\end{figure}

\paragraph{Punktdiagram}
Dette består av noder som er spredt rundt i et område. 
Posisjonen til en node kan fortelle noe om nodens egenskaper, på samme måte som en graf.
Et eksempel vises i figur \ref{scatterplot}.
\begin{figure}[ht]
\centering
\includegraphics[scale=0.5]{Scatterplot2.PNG}
\caption{Punktdiagram} \label{scatterplot}
\end{figure}
\paragraph{Fargekodete kart}
Ved å legge noder i et plan, og deretter fargelegge de delene som oppfyller et kriterie, vil man kunne få noe som minner om et kart, der interessante områder blir uthevet. Eksempelet i figur \ref{heatmap} viser gjennomsnittlig kvadratmeterpris i forskjellige bydeler i New York \cite{heatmapEks} .
\begin{figure}[ht]
\centering
\includegraphics[scale=0.5]{heatmap.jpg} 
\caption{Fargekodet kart} \label{heatmap}
\end{figure}

\paragraph{Tre-visualisering}
Kan vise nodens indre struktur og gjør det mulig å sammenligne to eller flere trær.
Jamført nodenes struktur, vil ikke dette være et typisk binærtre.
En mulighet vil også være å forsøke å visualisere en eller flere ``standardnoder'' basert på statistisk analyse av dataene, og så sammenligne enkeltnoder opp mot standarden.
Vi ønsker å kombinere flere av disse teknikkene der dette er mulig, for å kunne trekke ut informasjon og sammenhenger som ellers er vanskelige å finne.

\subsection{Hva skal visualiseres?}
For det første var det nødvendig å få oversikt over dataene, for å se hva de representerte. Etter hvert ble det klart at for å avgrense oppgaven, begrenset vi oss til components-seksjonen i filene.
Til å hjelpe oss med analysen, laget vi et Perl-script som gikk gjennom alle profil-filene, og fant antallet forskjellige komponenter, samt hvor mange maskiner som hadde disse.
Dette gjorde det enklere å få oversikt over hvilke komponenter som var i bruk.
Etter å ha lest gjennom dokumentasjonen over komponentene og sett hvilke parametere som var vanlige, ble det satt opp en liste over aktuelle felter i samarbeid med oppdragsgiver/veileder.
Eksempler på komponenter:
\begin{itemize}
\item inv/os -$>$ Operativsystem
\item inv/location -$>$ Fysisk lokasjon
\item apache -$>$ Betyr at maskinen kjører en webservertjeneste.
\item network/gateway -$>$ default gateway for maskinen.
\end{itemize}
Siden oppgaven var relativt løst formulert, hadde vi vanskeligheter med å slå oss til ro med de komponentene vi hadde valgt. 
Derfor konsentrerte vi oss heller om å lage generiske visualiseringer som kunne brukes til alle slags datafelter.
%DBI (Database Interface) er et databasegrensesnitt for Perl, som vi valgte å bruke for å lage og eksekvere spørringer til en database. Vi hadde liten forkunnskap om modulen, men syntaksen var lett å skjønne.  DBI-modulen kan koble seg opp mot mange databasemotorer på en enkel måte, og spørringer og svar er enkelt å fremstille og hente.

\section{Utviklingsprosessen}
Her vil vi forklare hvilke utviklingsfaser prosjektet har vært gjennom, utfordringer vi møtte underveis, og begrunnelser for valgene som ble tatt.
\subsection{Utviklingsfasene}

\subsubsection{Oppstart}
Gruppen ble samlet i oktober, bestående av tre personer som tidligere har jobbet sammen i forskjellige skoleprosjekter. 

For å komme fram til en passende oppgave, søkte vi etter potensielle prosjekter, og diskuterte hva som ville egne seg best for oss. Vi bestemte oss for dette prosjektet av flere grunner, men i hovedsak fordi det ville bli en veldig lærerik prosess å jobbe med nye emner vi har liten erfaring med.

\subsubsection{Planlegging}
Etter at oppgaven var tildelt fra arbeidsgiver, begynte planleggingen av prosjektet. Sammen med veileder satte vi opp en midlertidig kravspesifikasjon og rådførte oss om hvilke teknologier og verktøy som bør brukes. 

\subsubsection{Forstudie}
Bygging av kompetanse har vært vesentlig i dette prosjektet, og det ble satt av god tid til dette i denne fasen. 
Temaene som ble studert og prøvd ut var blant annet XML, Perl, og VRML.

\subsubsection{Prosessmodell}
Gruppa bestemte seg tidlig for å benytte seg av en smidig utviklingsprosess. Kravspesifikasjonene vil være i konstant utvikling, og derfor er det ønskelig å dra nytte av en iterativ utviklingsprosess. Selve programutviklingen vil iterere  fire faser; innledning, utforming, bygging og testing. Disse fasene kan overlappe hverandre. Siden gruppen hadde liten forkunnskap om teknologiene i prosjektet, var det viktig å komme straks i gang med programmeringen i prosjektet, og så bli kjent med språkene og verktøyene underveis.

I \textbf{innledningsfasen} planla og designet vi ny funksjonalitet på papir.

I \textbf{utformingsfasen} lagde vi en prototyp over ny funksjonalitet, og denne ble designet digitalt. Dette ble vanligvis gjort ved å redigere en VRML-fil for hånd.

I \textbf{byggingsfasen} integrerte vi generisk funksjonalitet for prototypen i systemet. 

I \textbf{testfasen} ble den nye funksjonaliteten testet. Ved eventuelle feil eller mangler gikk vi tilbake til utformingsfasen.

Disse fire fasene var spesielt viktige i utformingen av de forskjellige visualiseringsteknikkene våre.

\subsubsection{Dokumentasjon}
Etter at utviklingstiden var slutt, startet arbeidet med å ferdigstille dokumentasjonen. Mye av prosessdokumentasjonen har blitt fylt ut underveis, mens den resterende sluttdokumentasjon måtte skrives.
%\subsection{Metodikk / prosessmodell}
%Gruppen bestemte seg tidlig for å benytte seg av en smidig utviklingsprosess, med mange iterasjoner og konstant utvikling av kravspesifikasjonene.
%Vi valgte å bruke RUP (Rational Unified Process) som prosessmodell, med innslag av XP (eXtreme Programming).
%XP-elementer ble valgt blant annet fordi vi måtte komme raskt i gang med programmeringen, for å gjøre oss kjent med språkene, vi synes også parprogrammering kan være gunstig innimellom.
%\textbf{Testdrevet utvikling...?}
%
%Måten vi jobbet på for hver visualisering: Til å begynne med laget vi en prototyp kun med VRML-editor på hva vi ønsket oss.
%Så gikk vi over til å lage et perl-script der vi laget metoder for å generere ulike vrml-elementer og skrev så resultatet fra kjøringen ut til fil.
%Deretter inspiserte vi den genererte koden og sjekket at den virket i en vrml-viewer. Ved syntax-feil rettet vi opp vrml'en og la evt. til kode manuelt, testet dette og gikk så tilbake til scriptet og endret / la til metoder her som så genererte ny vrml-kode.

%\subsection{Faser}
%Fasene i prosjektet fulgte RUP-fasen, med mange iterasjoner for å forbedre og utvikle koden.\\
%Funksjonaliteten ble utvidet gjennom en inkrementell prosess, der vi startet med kun en visualiseringsteknikk og gjorde denne ferdig, før vi startet på en ny.
%
%Innledning\\
%Utforming\\
%Bygging\\
%Overgang





\subsection{Oppbygging av programmet}
Programmet består av to hoveddeler: dataImport og dataVisualiserer.
Vi har valgt å implementere en 3-lagstruktur med DAL (Data Access Layer) for database-tilkobling, BLL (Business Logic Layer) for generering av VRML, og GUI (Graphical User Interface)  for kommunikasjon/presentasjon for sluttbruker.
Dette er gjort for å gjøre systemet utvidbart og generisk - eksempelvis vil det være mulig å gå over til en annen databasemotor ved kun å endre/bytte ut DAL.
Videre har vi valgt å implementere løsningen som en web-applikasjon, da kombinasjonen $*$AMP (Apache, MySQL, Perl) er en god og plattformuavhengig kombinasjon, samt at det ikke stiller store krav til klienten, som egentlig kun trenger å kunne vise HTML, Javascript og VRML.
%\begin{figure}[ht]
%\centering
%\label{systemskisse}
%\includegraphics[height=8cm]{systemskisse.JPG} 
%\caption{Skisse av systemets oppbygging}
%\end{figure}
%\newpage
%\noindent
%BLL er delt opp i mindre moduler for å gjøre det enkelt å legge til funksjonalitet.  %Denne delen er så delt opp i moduler for hver komponent som skal trekkes ut, dette for å gjøre det enkelt å utvide importen til å gjelde flere komponenter / felter. Forhåpentligvis gjør dette også det lettere å endre konfigurasjonen til å kunne importere andre XML-filer, dersom andre senere vil importere andre typer filer.
\subsection{Dataimporter}
Importdelen er ansvarlig for å lese inn nye datasett i form av XML-filer, trekke ut ønskede felter og legge disse inn i databasen.
\subsubsection{XML-tolker} %TODO: Putte dette et skikkelig sted
For å tolke XML-filene, trenger Perl moduler for å forstå XML-struktur på en hensiktsmessig måte. Vi valgte først å bruke XML::DOM (Document Object Model) til å tolke disse filene. Da vi ble godt kjent med syntaksen lagde vi et testskript i Perl, og testet modulen på et lite sett med filer, og ekstrahering av informasjon fungerte godt. Ved tolking av et helt datasett, viste DOM seg å være veldig treg, og minneforbruket ble så stort at testmaskinene kræsjet. 
Vi løste midlertidig dette problemet ved å kalle en \verb"doc->dispose()" metode for hver fil vi hadde lest inn, fordi Perls \verb"Garbage Collector" ikke selv gjorde dette. Minneforbruket forholdt seg stabilt, men tolkingen tok fortsatt for lang tid. Vi søkte etter en ny modul, og fant LibXML som innfridde de forventninger vi har til en XML-tolker. Dette er en XML tolker til Gnome biblioteket (et Linuxgrensenitt), og viste seg å være utrolig rask. Syntaksen på XML-spørringene er litt annerledes fra DOM, så noe omskriving måtte til. Som nevnt må vi tolke et stort antall XML-filer, og LibXML viste seg å være raskere og mer effektiv enn DOM. Vi testet de to modulene opp mot hverandre, og fant ut at DOM forsyner seg av alt tilgjengelig systemminne mens LibXML har et stabilt og lavt minneforbruk. I tillegg viste tester at LibXML omtrent ti ganger kjappere.

\subsubsection{Import til database}

\subsubsection*{Planlegging}

Da vi planla hvordan import til databasen skulle foregå, tenkte vi å lage egne Perl-moduler som inneholder spesifikasjon av hvilke komponenter som skal bli ekstrahert fra XML-filene, og definisjoner på hvordan de skal legges inn i databasen. Produktet vil bli levert med noen standard moduler, og det skal være mulig for brukeren å lage egne. Disse modulene er en god løsning for å trekke ut data, og de vil gi brukeren god konfigurasjonsfrihet.

Disse Perl-modulene var en idé som måtte forkastes. Det vil bli for mye jobb for brukeren å skreddersy Perl-moduler etter behov. Derfor innføres heller en sentral konfigurasjonsfil, hvor det viktigste av konfigurasjon av programmet skal bli satt. Denne konfigurasjonsfilen vil inneholde informasjon om databasetilkoblingen, XML-filene og hvilke felter som skal importeres og visualiseres.

\subsubsection*{Implementasjon og metode}

Perl-scriptet \verb"XML_to_DB" er programmet som blir brukt til å importere XML-verdier til databasen. Det leser informasjon fra konfigurasjonsfilen, oppretter databasetilkobling, ekstraherer ønsket komponentinformasjon fra XML-filene, og sender XML-verdier til databasen via DAL.

\subsubsection*{Databasestruktur}
Etter å ha analysert XML-filene, kom vi fram til at det ville være hensiktsmessig å opprette en tabell for hver konfigurasjonsdel som vi ønsker å importere. Et eksempel: i XML-filene har vi som oftest en konfigurasjonsdel \verb"<network>". Barnenoder av network er for eksempel \verb"<gateway>", så dersom vi ønsker å importere denne taggen, oppretter vi en tabell \verb"network", med et felt 
\verb"gateway". Primærnøkkelen ble satt til navnet på XML-fila (som tilsvarer maskinnavnet), og som standard opprettes alle felt som \verb"VARCHAR(200)".

\subsubsection*{Nye datasett}
Etter at halve prosjektperioden var gått fikk vi tildelt nye datasett fra oppdragsgiver. Disse nye datasettene var tatt gjennom et tidsløp på nesten tre måneder, og vi måtte oppdatere våre metoder for å legge til nye verdier til databasen. 

Med nye datasett spiller dato en viktig rolle for visualisering. Primærnøkkelen i tabellene vil nå være maskinnavn kombinert med datoen spesifisert i filens \verb"<last_modified>"-tag, da konfigurasjonen kan endres over tid og vi trenger å ta vare på endringer i konfigurasjon. Siden det skal være mulig å legge til nye datafelter for import, vil databasen vår bli utvidet med flere tabeller og/eller nye felt etter behov.

Den obligatoriske komponenten \verb"<last_modified>" forteller når filen sist ble endret, men det betyr ikke at verdiene vi skal ha inn i databasen har fått ny verdi siden siste import. Derfor måtte det også implementeres en kryssjekk, som ser om det er redundant data som vil bli lagt til. Hvis det er blitt noen forandringer, vil det opprettes en ny rad med nye data.

Når XML-verdier blir sendt fra \verb"XML_to_DB" til DAL, har det blitt opprettet en databasetilkobling for hver eneste spørring. Dette vil si at det har blitt gjort omtrent 2000 databasetilkoblinger for et datasett. Siden ekstrahering og import av data tar forholdsvis lang tid, ville vi minske tidsforbruket. Løsningen ble å gjøre DAL objektorientert, slik at tilkoblingen til databasen er åpen så lenge programmet kjører.

De nye datasettene innførte flere spesielle tegn i komponentenes verdier, for eksempel `;'. Disse tegnene ødelegger SQL-spørringene fra importerscriptet til DAL. For å løse dette ble det laget en metode som bruker substitusjon og regulære uttrykk for å fjerne disse tegnene.

\subsection{Datavisualiserer}
dataVisualiserer-delen har ansvaret for å hente data fra databasen og generere VRML som så blir presentert for brukeren. 


\subsubsection{Visualisering i praksis} %Evt. programmering... 
Mye av prosjekttiden har blitt brukt til å lære om og prøve ut forskjellige teknikker, og utvikle egne algoritmer gjennom eksperimentering. 
Til å begynne med resulterte dette i enkle visualiseringer av noder, der et kriterium bestemmer utseendet til noden. Etter hvert kunne flere av disse metodene settes sammen, og dermed danne mer komplekse visualiseringer ved å kombinere eksisterende metoder.
Det var også interessant å finne algoritmer som hensiktsmessig plassererer objekter i et tredimensjonalt rom. 
Da det ikke er mulig å forutsi hvor mange noder eller grupper som kan forekomme i en visualisering, måtte det utvikles generiske metoder. 

\subsubsection{Brukergrensesnitt (GUI) }
Vi ble enige om at vi ville ha et brukergrensesnitt som er enkelt og funksjonelt, hvor hovedfokuset skal være på visualiseringsfilen. Med en browser-basert løsning har vi muligheten til å gi brukeren et enkelt grensesnitt, og VRML-fila kan legges inn og oppdateres kontinuerlig. Vi valgte å bruke en av Perl's moduler, CGI (Common Gateway Interface), til å lage dette. CGI kommuniserer med Perl og Apache på en god måte, og siden det er laget i Perl trenger vi ikke introdusere nye språk/teknologier i brukergrensesnittet.
Hendelsesforløpet ved å få fram en visualisering i GUI er tenkt slik:
\begin{itemize}
\item Velge visualiseringsteknikk
\item Velge tabeller og kriterier
\item VRML-filen blir lagt til, og kommer opp på siden
\end{itemize}
Selve VRML-fila (.wrl) som ble lagt til i websiden ble ikke oppdatert i nettleseren. Ved første kjøring av websiden fungerte det å hente opp den genererte VRML-fila, men ved neste kjøring (med helt andre kriterier), ble fortsatt den gamle VRML-fila vist. Tømming av cache hjalp ikke. Etter mye testing viste det seg at nevnte feil er nettleserrelatert. 
Ved omstart av nettleseren blir riktig VRML-fil lastet opp på websiden. 
Løsningen ble å lage en lenke til den genererte fila, slik at brukeren manuelt kan åpne den nyeste visualiseringen.

\subsection{Utfordringer}

Det har utvilsomt vært utfordringer underveis. Først og fremst har det vært problematisk å tilegne seg kunnskap om VRML97. Det er et litt eldre språk, og det er mye utdatert stoff både i bøker og på Internett. Dette omhandlet ofte gamle spesifikasjoner eller gjaldt for utdaterte VRML-lesere, som gjorde det vanskelig å finne ut hva som gjelder nå. I tillegg har det vært en bratt læreingskurve for å beherske det. De ulike VRML-Browserne fulgte ikke nødvendigvis de samme spesifikasjonene, slik at gyldig VRML-kode kunne oppføre seg forskjellig i to programmer. Derfor valgte vi å standardisere etter Octaga Player, siden den virker mest robust og ferdig.

\subsubsection*{Beregning av 3D-posisjoner}
En viktig del av visualiseringen har vært å finne algoritmer som kan brukes til beregning av posisjoner i det tredimensjonale rommet. Siden posisjonene i VRML er oppgitt i kartesiske koordinater, har det vært hensiktsmessig å benytte vektorer for å beregne plasseringen av objektene. 
I visualiseringene har vi funnet det nødvendig å bruke to metoder for å generere vilkårlige koordinater. Den ene genererer en vilkårlig posisjon innenfor en boks med gitte dimensjoner, og den andre genererer en posisjon mellom to sfærer som begge har sentrum i origo. Begge metodene returnerer en array på tre elementer som representerer en tredimensjonal vektor.\\
Metoden for beregning av posisjon innenfor en boks returnerer en array med vilkårlige verdier mellom null og angitt maksimalverdi for henholdsvis bredde, høyde og dybde.

Metoden for beregning av koordinater mellom to sfærer er litt mer komplisert. Den trenger to parametere som angir radius på den indre og den ytre sfæren. I tillegg tar den en parameter som angir hvilken avstand som kan brukes til å gi en skalering av avstanden mellom to posisjoner. Metoden genererer først en tilfeldig vektorlengde som ligger mellom de to grensene angitt ved de to første parametrene delt på avstandsparameteren. Denne vektorlengden representerer radius r i likningen for en kule med senter i origo der x, y og z er aksene: \newline
\begin{equation} r^{2} = x^{2} + y^{2} + z^{2}\end{equation}
X-verdien settes først til et tilfeldig tall slik at $ x \in [ \: 0,  r \: ] $. Deretter settes y tilfeldig slik at
$y \in [ \: 0,  \sqrt{ r^{2} - x^{2} } \: ] $
før z-verdien til slutt beregnes ut fra r, x og y:
\begin{equation}z = \sqrt{ r^{2} - (x^{2} + y^2) }\end{equation}
De tre beregnede verdiene for henholdsvis x, y og z er alle positive flyttall. De representerer derfor kun punkter i første kvadrant. For å få negative verdier, og dermed kunne fylle en hel sfære, blir hver av verdiene tilfeldig multiplisert med 1 eller -1. I tillegg multipliseres hver av verdiene med avstandsparameteren for å få tilbake ønsket skalering, og verdiene konverteres til heltall før de returneres som en array på tre elementer.

\subsubsection*{Generering av farger} 
VRML krever at en farge beskrives som en tredimensjonal vektor, der hver komponent i vektoren er et tall $ x \in [ 0, 1 ] $.
Komponentene $x_1$ , $x_2$  og $ x_3 $ representerer primærfargene rød, grønn og blå, og ved å endre verdiene på en eller flere av disse kan man generere ulike farger fra $ [0,0,0] $ (svart) til $ [1,1,1] $ (hvit).
I en visualisering er det ofte naturlig å fargelegge ulike komponenter eller grupper for å markere hva som hører sammen. Da er det en fordel om fargene på forskjellige gruppene ikke er altfor like. 
Dette blir vanskeligere hvis det er svært mange grupper, så fra et visst punkt er det ikke lenger praktisk å bruke farger for å vise forskjeller.
Ved å prøve oss fram med forskjellige teknikker, har vi endt opp med å bruke en metode som genererer opp til 36 ulike farger. 
Dette er også kombinert med å gi sluttbruker mulighet til å filtrere ut grupper helt for å øke lesbarheten. 

\subsection{Om kravspesifikasjonen}
I begynnelsen av prosjektperioden hadde prosjektet en svært løs kravspesifikasjon, blant annet basert på samtaler med veileder og oppdragsgiver (se \ref{mail}).
Underveis i prosjektperioden ble disse kravene raffinert, og nye krav ble lagt til.
Dette ble gjort fordi vi trengte kunnskap om både verktøy og konfigurasjonsfiler  for å kunne sette realistiske krav.
Kravspesifikasjonen har dermed vært i konstant utvikling gjennom hele prosjektperioden, og det er gruppemedlemmene selv som har utarbeidet denne.
Dette har gjort arbeidet spennende, fordi gruppen har fått stor grad av frihet til å selv velge ut interessante oppgaver og bestemme hvordan disse skal løses.
Arbeidet uten en tydelig kravspesifikasjon har også gjort det til en større utfordring. I motsetning til et prosjekt med strenge krav, har vi måttet bruke mye tid på å finne ut hva vi skal gjøre, i tillegg til hvordan det best kan løses. En ulempe med dette, var at det var vanskelig å legge langsiktige planer, samt å vite hvordan vi lå an i forhold til tid. 
Vi følte også mange ganger at det faktiske arbeidet vi gjorde, var langt mer omfattende enn det endelige produktet skulle tilsi.
\section{Avslutning}

\subsection{Oppsummering}
Prosjektperioden har vært utfordrende og lærerik.
Ved prosjektstart hadde vi bare en vag ide om hva vi skulle gjøre og hvordan, og fra dette har vi kommet et stort skritt videre.
Gjennom arbeidet har vi lært mye, spesielt om visualisering, 3D og Perl, og føler at dette grunnlaget kan komme godt med i mange sammenhenger.
Vi synes også at det har vært gøy å jobbe med et litt ``annerledes'' prosjekt, med stor grad av frihet og mulighet til å utforme løsninger selv.


\subsection{Hva kunne vært gjort annerledes}
Under utviklingen ble det lagt vekt på å lage ting så generisk som mulig, samt at  visualiseringene skal fungere godt.
Punkter til forbedring i en neste iterasjon, er blant annet datastrukturen:  
Databasen skulle vært redesignet, for å få den normalisert og mer hensiktsmessig bygget opp. 
Det hadde også vært bra å få utvidet visualiseringsteknikkene til å inkludere muligheten for å visualisere utvikling over tid, i nåværende versjon er det kun en teknikk som gjør dette.
Vi ville også ha forsøkt å finne ut hvordan vi kunne ha produsert VRML-kode som virker feilfritt i andre VRML-browsere.

Mot slutten av prosjektperioden kunne vi begynne å høste fruktene av alt vi hadde gjort og lært, så det var lettere å utvikle ny funksjonalitet og se mulighetene. Aller helst skulle vi ha vært på det stadiet litt tidligere, så kanskje vi burde ha prioritert å arbeide mindre med dataene og mere med VRML til å begynne med.



\subsection{Konklusjon}
***Her kommer noen svulstige ord, og takk for nå..



\bibliography{kilder} 
\bibliographystyle{plain} 

\section{Appendix}


\subsection{E-post fra Paul Anderson}

\label{mail} 
\begin{verbatim}
I'm enclosing one XML "profile" which is the entire configuration spec
for one machine (my desktop in this case). We have about 1500 of these
for the whole of our real installation, and we could probably also
extract a few years worth of historical ones from the CVS.

As I said, this should provide a fairly explicit format for extracting
data which we could visualise, but there will probably be some wrinkles
in practice. In case you feel like looking it over, here are a few
random initial comments ...

(*) the <components> section is probably all you want to look at.

(*) This has a list of configuration components - eg. the first one
called "LPRng"

(*) Each component has a list of key/value pairs ("resources") - eg.
buildperms = /usr/sbin/build-print-perms

(*) You can throw away the "derivation" attribute - this tells you where
in the source it is generated from (for debugging)

(*) Some resource values are compound structures called "tag lists"
which are like ordered records
    In this case, the "name" attribute gives the "tag" (record name). Eg.
    conf.printcap = printcap_path=| -\$ /usr/bin/pcap-query
    conf.nonprintable = check_for_nonprintable@
    Etc ....

(*) These structures can be nested - the XML for this probably looks a
bit odd.

(*) You can throw away the "template" attribute - this is used for
serialising these structures and forming variable names.

(*) I suspect that these structures will cause the most trouble in
practice, because sometimes the tags are significant and their values
are used by the component. Other times, the tags are arbitrary and it is
only the values and the order which are significant. This is a
consequence of using a single data structure ("tag list") to represent
both lists and records. This is historical, and I'm not sure whether it
is still a good idea or not! In practice, we could probably create a
kind of schema file for each component type saying which things were
significant.

(*) I'd be very interested in trying to visualise how "different" the
machines are. The metric for this would presumably be something simple
to start with like the number of resources which were different between
two machines. Once we see what this looks like, we would probably want
to refine this, for example with weightings. We might even want to play
with this dynamically.

(*) I suspect that a large number of the resource values never vary at
all for machines at our site - it would be interesting to know what
proportion - but these can probably just be thrown away (once they have
been identified).

(*) The values are nearly all going to be strings. Even when they are
numeric, I don't think it will make any sense to treat them as numbers
when computing any kind of metric.

(*) Some resource - eg. conserver.servers specify the names of other
machines. It should be possible to identify these resources and use
(only) those to identify dependencies between clients and servers (for
different services). This might make another interesting visualisation!

-----------------

Paul fremhever spesiellt to ting han er interessert i å visualisere:

1) Dependencies - elements in the configuration for one machine
reference other machines - this forms a dependency network.

2) Clustering - it would be nice to be able to compare the configuration
resources for machines and group the machines by how "similar" their
resources were, and then visualise the resulting clusters.


----
\end{verbatim}

\end{document}


